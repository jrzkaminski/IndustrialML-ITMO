{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libs, data importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plots\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['lines.linewidth'] = 1.5\n",
    "%matplotlib inline\n",
    "\n",
    "# Modelling and Forecasting\n",
    "# ==============================================================================\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from rich.progress import track\n",
    "from datetime import date\n",
    "from joblib import dump, load\n",
    "\n",
    "# Configuration\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "import holidays\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('/home/jerzy/Documents/IndustrialML/data/train_preprocessed.csv')\n",
    "data_train.drop('Unnamed: 0', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('/home/jerzy/Documents/IndustrialML/data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = pd.read_csv('/home/jerzy/Documents/IndustrialML/data/valid.csv')\n",
    "# drop row 0101000020E610000000000000000000000000000000000000\n",
    "data_val = data_val[data_val['point'] != '0101000020E610000000000000000000000000000000000000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_unique_points = data_train['point'].unique()\n",
    "test_unique_points = data_test['point'].unique()\n",
    "valid_unique_points = data_val['point'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into year, month, day, hour and adding holidays, weekends, weekdays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add datetime column to test and valid datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['datetime'] = pd.to_datetime(data_test['hour'], unit='s')\n",
    "data_val['datetime'] = pd.to_datetime(data_val['hour'], unit='s')\n",
    "data_train['datetime'] = pd.to_datetime(data_train['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add yyyy, mm, dd, hh to test and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[\"dayhour\"] = data_train[\"datetime\"].dt.hour\n",
    "data_train[\"weekday\"] = data_train[\"datetime\"].dt.weekday\n",
    "\n",
    "data_test[\"dayhour\"] = data_test[\"datetime\"].dt.hour\n",
    "data_test[\"weekday\"] = data_test[\"datetime\"].dt.weekday\n",
    "\n",
    "data_val[\"dayhour\"] = data_val[\"datetime\"].dt.hour\n",
    "data_val[\"weekday\"] = data_val[\"datetime\"].dt.weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_russia = holidays.country_holidays('RU', years = [2019, 2020])\n",
    "\n",
    "data_train['is_holiday'] = data_train['datetime'].apply(lambda x: 1 if x in holidays_russia else 0)\n",
    "data_test['is_holiday'] = data_test['datetime'].apply(lambda x: 1 if x in holidays_russia else 0)\n",
    "data_val['is_holiday'] = data_val['datetime'].apply(lambda x: 1 if x in holidays_russia else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.drop(['time'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressions: catboost, lightgbm and xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(11, 4))\n",
    "\n",
    "# data_train[data_train['point'] == '0101000020E6100000DBC1F19351543E4006FC5DE561F84D40']['num_posts'].plot(ax=ax, label='train')\n",
    "# data_val[data_val['point'] == '0101000020E6100000DBC1F19351543E4006FC5DE561F84D40']['sum'].plot(ax=ax, label='validation')\n",
    "# data_test[data_test['point'] == '0101000020E6100000DBC1F19351543E4006FC5DE561F84D40']['sum'].plot(ax=ax, label='test')\n",
    "# ax.set_title('Number of users')\n",
    "# ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_metric(y_true, y_pred):\n",
    "    return abs(y_true - y_pred) / y_true\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def learn_regressor(data_train: pd.DataFrame, data_test: pd.DataFrame, model):\n",
    "    data_test_boost_error = []\n",
    "    test_unique_points = data_test['point'].unique()\n",
    "    for point in track(test_unique_points, description='Learning'):\n",
    "        model = model\n",
    "        data_train_point = data_train[data_train['point'] == point]\n",
    "        data_test_point = data_test[data_test['point'] == point]\n",
    "        X_train = data_train_point[['timestamp', 'dayhour', 'weekday', 'is_holiday']]\n",
    "        y_train = data_train_point['num_posts']\n",
    "        model.fit(X_train, y_train)\n",
    "        data_test_point['boost'] = np.round(model.predict(data_test_point[['hour', 'dayhour', 'weekday', 'is_holiday']].values))\n",
    "        data_test_point['boost_error'] = custom_metric(data_test_point['sum'], data_test_point['boost'])\n",
    "        data_test_boost_error.append(list(data_test_point['boost_error']))\n",
    "    return data_test_boost_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual info score calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutual information matrix\n",
    "# =================================================================================\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "def mutual_info_matrix(data):\n",
    "    mi_matrix = pd.DataFrame(index=data.columns, columns=data.columns)\n",
    "    for i in track(data.columns, description='Calculating MI matrix'):\n",
    "        for j in data.columns:\n",
    "            mi_matrix.loc[i, j] = round(float(mutual_info_score(data[i], data[j])), 2)\n",
    "    return mi_matrix\n",
    "\n",
    "mi_train = mutual_info_matrix(data_train)\n",
    "\n",
    "# convert all columns to float \n",
    "mi_train = mi_train.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))  \n",
    "sns.heatmap(mi_train, annot=True, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation heatmap\n",
    "# =================================================================================\\\n",
    "corr = data_train.corr()\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(corr, annot=True, fmt='.2g',cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test data\n",
    "\n",
    "time = 36.2 s\n",
    "\n",
    "error = 0.86883"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_test_xgboost_error = learn_regressor(data_train, data_test, XGBRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_xgboost_error = flatten(data_test_xgboost_error)\n",
    "data_test['xgboost_error'] = data_test_xgboost_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['xgboost_error'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validation data\n",
    "\n",
    "time = 37.2 s\n",
    "\n",
    "error = 0.86883"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_val_xgboost_error = learn_regressor(data_train, data_val, XGBRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val_xgboost_error = flatten(data_val_xgboost_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val['xgboost_error'] = data_val_xgboost_error\n",
    "data_val['xgboost_error'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test data\n",
    "\n",
    "time = 121.6 s\n",
    "\n",
    "error = 0.86764"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_test_catboost_error = learn_regressor(data_train, data_test, CatBoostRegressor())\n",
    "data_test_catboost_error = flatten(data_test_catboost_error)\n",
    "data_test['catboost_error'] = data_test_catboost_error\n",
    "data_test['catboost_error'].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validation data\n",
    "\n",
    "time = 124.9 s\n",
    "\n",
    "error = 0.87005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_val_catboost_error = learn_regressor(data_train, data_val, CatBoostRegressor())\n",
    "data_val_catboost_error = flatten(data_val_catboost_error)\n",
    "data_val['catboost_error'] = data_val_catboost_error\n",
    "data_val['catboost_error'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test data\n",
    "\n",
    "time = 30.2 s\n",
    "\n",
    "error = 0.866203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_test_lgdmboost_error = learn_regressor(data_train, data_test, LGBMRegressor())\n",
    "data_test_lgdmboost_error = flatten(data_test_lgdmboost_error)\n",
    "data_test['lgdmboost_error'] = data_test_lgdmboost_error\n",
    "data_test['lgdmboost_error'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validation data\n",
    "\n",
    "time = 30.5 s\n",
    "\n",
    "error = 0.868838"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_val_lgdmboost_error = learn_regressor(data_train, data_val, LGBMRegressor())\n",
    "data_val_lgdmboost_error = flatten(data_val_lgdmboost_error)\n",
    "data_val['lgdmboost_error'] = data_val_lgdmboost_error\n",
    "data_val['lgdmboost_error'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('indml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02902d8ec977eb7ac2114c5b569c3cebeb1ab04bf847c131eacd9d4311affd20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
